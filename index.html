<!DOCTYPE html>
<html>

<head>
  <title>Double Y: Tropical storm damage detection</title>
  <link rel="icon" type="image/png" href="static/images/EY Satelite Image.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <meta name="description" content="Double Y: Tropical storm damage detection">
  <meta property="og:title" content="Double Y: Tropical storm damage detection"/>
  <meta property="og:description" content="AUTOMATING COASTAL VULNERABILITY ASSESSMENT"/>
  <meta property="og:url" content="https://github.com/Double-Y-EY-Challenge-2024"/>
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Post-Storm Event Assessment: Damaged Building Detection 
              and Classification</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/wongyijie/" target="_blank">Yi Jie WONG</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yinloonkhor/" target="_blank">Yin Loon KHOR</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ziweiliu2023/" target="_blank">Ziwei LIU</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><strong>Group Name:</strong> Double-Y | <strong>Public
                  Leaderboard:</strong> 11/222 (Top 5%)<br>
                <a href="https://challenge.ey.com/challenges/tropical-cyclone-damage-assessment-lrrno2xm">EY Open
                  Science Data Challenge Program 2024</a>
              </span>

              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- PDF -->
                <span class="link-block">
                  <a href="static/pdfs/Team Double Y - Approach Document.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                
                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Double-Y-EY-Challenge-2024/EY-challenge-2024" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Source Code</span>
                  </a>
                </span>

                <!-- Best model -->
                <span class="link-block">
                  <a href="https://github.com/Double-Y-EY-Challenge-2024/EY-challenge-2024/blob/main/best-trained-model.pt"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Best Model</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


<!-- Teaser GIF -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/heatmaps/movie.gif" alt="Inference samples predicted by our trained model.">
      <h2 class="subtitle">
        Inference samples predicted by our trained model.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser GIF -->



  <!-- Summary -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Summary</h2>
          <div class="content has-text-justified">
            <p> The 2024 EY Open Science Data challenge focuses on coastal resilience and climate change. Participants 
              will use high-resolution satellite datasets to build predictive models to help vulnerable coastal 
              communities adapt to evolving conditions and recover from extreme climate events. These solutions will 
              result in new and innovative ideas designed to increase the impact of data for societal benefit.
            </p>

            <p>Despite these challenges, we are immensely grateful, and our collective efforts enabled us to achieve a
              final mAP50 score of 0.51, placing us in the top 5% in the public leaderboard.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Competition Overview -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Competition Overview</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>This overview is adapted from the original competition description.</p>
          <p><strong>1. Objective:</strong> The objective of the challenge is to develop a machine learning model
            capable of identifying and detecting "damaged" and "undamaged" coastal infrastructure, including residential
            and commercial buildings, affected by natural disasters such as hurricanes and cyclones.</p>
          <p>Participants will be provided with pre- and post-cyclone satellite images of an area impacted by Hurricane
            Maria in 2017. The task is to build a machine learning model capable of detecting four different types of
            objects in the satellite images of cyclone-affected areas:</p>
          <ul>
            <li>Undamaged residential buildings</li>
            <li>Damaged residential buildings</li>
            <li>Undamaged commercial buildings</li>
            <li>Damaged commercial buildings</li>
          </ul>
          <p><strong>2. Dataset Used:</strong></p>
          <p><strong>Mandatory dataset:</strong></p>
          <ul>
            <li>High-resolution panchromatic satellite images before and after a tropical cyclone: Maxar GeoEye-1
              (optical)</li>
          </ul>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Competition Overview -->

  <!-- Key Challenges -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Key Challenges</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>1. Dataset Collection:</strong> Manually annotating all four classes in the provided high-resolution
            satellite dataset from Maxar's GEO-1 mission, covering an area of 327 sq.km of San Juan, Puerto Rico, is a
            time-consuming task. With only one month for the competition duration, this task poses significant
            challenges in terms of time and energy allocation.
          </p>
          <p>
            <strong>2. Imbalanced Dataset:</strong> The dataset comprises four classes of building damages: undamaged
            commercial buildings, undamaged residential buildings, damaged commercial buildings, and damaged residential
            buildings. Our analysis reveals that damaged classes are underrepresented compared to undamaged ones.
            Additionally, residential buildings are more common than commercial buildings. This imbalance can introduce
            bias and affect the performance of our model, particularly towards the majority class.
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Key Challenges -->

  <!-- Proposed Approach -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Proposed Approach</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>1. Parallel Annotation:</strong> Instead of annotating the entire dataset as a single unit, we adopt
            a strategy of segmenting the dataset into multiple tiles, each with a dimension of 512x512 pixels. We chose
            this specific dimension to closely match the size of the validation dataset, ensuring consistency between
            our training and evaluation processes. These tiles are distributed among team members to enable parallel
            annotation, thereby enhancing efficiency.
          </p>
          <p>
            <strong>2. Annotation of Mixed-Area Tiles Only:</strong> Our analysis of the validation dataset indicates
            that it covers regions with mixed land use, covering both commercial and residential areas. Therefore, we
            focus our annotation efforts solely on regions characterized by this mixed land use, optimizing resource
            utilization and annotation accuracy.
          </p>
          <p>
            <strong>3. Cross-Checking Building Classes for Accurate Labeling:</strong> Annotating building classes
            accurately poses a significant challenge, particularly without reliable reference points and solely rely on
            guessing. To address this, our approach incorporates the generation of Google Maps and OpenStreetMap tiles,
            aligned with the center coordinates of each dataset tile. Additionally, we utilize shapefiles provided by
            the organizers to identify building footprints. Furthermore, we extract center coordinates for each tile and
            leverage them to access 3D views of the corresponding areas on Google Maps. This effort of cross-referencing
            process ensures the accuracy of our manual annotations by validating them against multiple reliable sources.
          </p>
          <p>
            <strong>4. Synthetic Image Generation for Handling Imbalanced Datasets:</strong> To mitigate the class
            imbalance in our dataset, particularly in representing damaged building classes, we propose employing a
            synthetic image generation technique. We draw inspiration from recent research outlined in a paper detailing
            the application of Stable Diffusion 2 with LoRa adaptation for generating synthetic aerial images. By
            fine-tuning this model, we aim to create synthetic images of damaged building classes, thus narrowing the
            representation gap in our training dataset and reducing model bias. The paper we reference in our
            replication efforts can be found at the following link: https://arxiv.org/abs/2311.12345
          </p>
          <p>
            <strong>5. Transfer Learning through Fine-Tuning of a Pre-trained Model:</strong> To enhance the performance
            of our model, we employ transfer learning by fine-tuning our dataset with a pre-trained model. Specifically,
            we utilize the YOLOv8 model developed by Ultralytics as our pre-trained model. Leveraging transfer learning
            allows us to capitalize on the knowledge gained by the pre-trained model on a related task, facilitating
            faster convergence and potentially improving the accuracy of our predictions.
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Proposed Approach -->

  <!-- Methodology -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Methodology</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>1. Data Collection:</strong>
            The dataset consists of pre- and post-event satellite imagery partitioned into multiple tiles. Initially in
            GeoTiff format, each tile is converted to JPG to reduce storage consumption. With a total of 21,460 tiles,
            including both pre- and post-event images, we generated OpenStreetMap and GoogleMap representations for each
            tile by aligning and matching raster layers in QGIS. Additionally, building footprint information was
            extracted by aligning shapefile raster layers with the GeoTiff dataset.

            Manual filtering was performed to select only images with mixed areas of residential and commercial zones.
            Furthermore, we extracted center coordinates for each tile and leveraged them to access 3D views of the
            corresponding areas on Google Maps. This enabled us to label all the buildings with high confidence by
            cross-checking building classes from multiple references.

            All tiles were then distributed among team members for manual annotation using VGG Image Annotator and in
            COCO annotation format. Data hosting and version control were managed through GitHub, utilizing Git for
            seamless working between local and remote repositories.

            At the end of the competition, we manually annotated a total of 820 images. These annotations amounted to
            11,344 labels, distributed as follows: undamaged residential building (7951), undamaged commercial building
            (2249), damaged residential building (907), and damaged commercial building (237).

          </p>
          <p>
            <strong>2. Data Pipeline:</strong> To streamline workflow efficiency, we developed a data pipeline for data
            processing and training dataset preparation. This pipeline receives JSON files containing COCO annotation
            format data and undergoes multiple stages of processing, including JSON to COCO PyLabel dataset
            transformation, dataset aggregation from multiple inputs, and dataset cleaning. The pipeline ultimately
            produces YOLO structured files optimized for training YOLO-based models.
          </p>
          <p>
            <strong>3. Training Machine Learning Model:</strong> Effective training of the machine learning model
            requires substantial hardware resources and CUDA acceleration for accelerated training. We trained our model
            by leveraging Kaggle's cloud platform, which offers generous GPU access of 30 hours per week. Fine-tuning
            was performed on YOLOv8 pre-trained models provided by Ultralytics, utilizing their high-level framework for
            efficient model training and evaluation without the need for extensive architectural considerations.
          </p>
          <p>
            <strong>4. Monitoring Machine Learning Training:</strong> WandB is used for experiment monitoring,
            integrated with YOLOv8 training. This facilitated efficient experimentation with
            various YOLO variants and hyperparameters, enabling us to track and evaluate multiple models effectively.
            WandB served as a benchmarking tool, aiding in the selection of optimal model configurations and identifying
            areas for improvement.
          </p>
          <p>
            <strong>5. Synthetic Image Generation:</strong> To generate synthetic images, we chose to fine-tune the
            Stable Diffusion 2 model by StabilityAI. Training dataset preparation involved extracting images with
            bounding boxes for each class from our manually annotated dataset, used to fine-tune Stable Diffusion 2 for
            synthetic image generation. Experimentation with different settings and hyperparameters, including LoRa
            scale, training epochs was conducted, monitored using WandB. The dataset and trained Stable Diffusion models
            are hosted on Hugging Face.
          </p>
          <p>
            <strong>6. Evaluating the Trained Model:</strong> Trained models were evaluated based on metrics such as
            mAP50, precision, and recall for each class. Confusion matrices, confidence curves, precision-recall curves,
            and other metrics were utilized to assess model performance. Evaluation was conducted on WandB, enabling
            comprehensive analysis to ensure effective predictions for each class.
          </p>
          <p>
            <strong>7. Prediction of Trained Model:</strong> Evaluation of the trained model involved testing on 12
            validation images provided by the organizers. Results from the detection from each of the validation image
            (class, confidence score, bounding box coordinates) are saved in a .txt file and all the .txt files are
            saved in a single .zip file. This zip file is uploaded to the challenge platform to get a score on the
            ranking board. To enhance scores, prediction configurations were adjusted to minimize bounding box overlap
            and optimize confidence thresholds.
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Methodology -->

  <!-- Preparing Data for Annotation -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Preparing Data for Annotation</h2>
      <div class="content has-text-justified">
        <div style="text-align: center;">
          <img src="static\images\preparing_data_annotation.jpg" alt="PrepareData" width="820">
          <p class="caption" style="width: 100%; text-align: center;"><b>Figure 1. Preparing data for annotation.</b>
            Workflow illustrating the process of preparing data prior to annotation.</p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Preparing Data for Annotation -->

  <!-- Annotation Process -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Annotation Process</h2>
      <div class="content has-text-justified">
        <div style="text-align: center;">
          <img src="static\images\annotation_process.jpg" alt="PrepareData" width="820">
          <p class="caption" style="width: 100%; text-align: center;"><b>Figure 2. Annotation process.</b> We utilized
            the VGG Image Annotator for annotating the tiles. Each building within the tiles is classified into one of
            four classes: undamaged residential building, damaged residential building, undamaged commercial building,
            or damaged commercial building. During annotation, we validated the annotations by comparing them to
            multiple resources. To determine whether a building is damaged, we compared it between pre- and post-event
            images, looking for indications of structural damage such as roof loss. To distinguish between commercial
            and residential buildings, we referred to resources such as Google Maps, OpenStreetMap, and Google Maps 3D.
            These sources provide information about place names, allowing us to infer whether a given building is
            commercial or residential. Additionally, having multiple references is beneficial in cases where the
            pre-event images are obscured by clouds; we compare the building against other resources to verify its
            classification.</p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Annotation process -->

  <!-- Data Pipeline -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Data Pipeline</h2>
      <div class="content has-text-justified">
        <div style="text-align: center;">
          <img src="static\images\data_pipeline.jpg" alt="PrepareData" width="820">
          <p class="caption" style="width: 100%; text-align: center;"><b>Figure 3. Data pipeline.</b> Data pipeline
            designed to enhance workflow efficiency for processing and preparing training datasets. Begins with the
            transformation of JSON files containing COCO annotation format data into PyLabel datasets. Subsequently, the
            pipeline aggregates datasets from multiple inputs and conducts a cleaning process. Ultimately, the pipeline
            generates YOLO structured files optimized for training YOLO-based models.</p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Data Pipeline -->

  <!-- Synthetic Image Generation -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Synthetic Image Generation</h2>
      <div class="content has-text-justified">
        <div style="text-align: center;">
          <img src="static\images\synthetic_generation.jpg" alt="PrepareData" width="820">
          <p class="caption" style="width: 100%; text-align: center;"><b>Figure 4. Synthetic Image Generation.</b> The
            representation of damaged datasets is notably low. To address the imbalance in the dataset, we implemented
            methods to balance the training dataset by generating synthetic images. We fine-tuned the Stable Diffusion 2
            model from StabilityAI using the training dataset obtained from manual annotations. Specific bounding boxes
            for classes were extracted from our training dataset and compiled into a Hugging Face dataset, which serves
            as input for training the Stable Diffusion 2 model. The trained model will then be utilized to generate
            synthetic images. These synthetic images will be merged with background images using Copy-Paste augmentation
            techniques. Our approach aims to replicate the methodology outlined in the referenced paper by Jian et al.
            (2023) titled <a href="https://arxiv.org/pdf/2311.12345.pdf">Stable Diffusion For Aerial Object
              Detection</a>. </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Synthetic Image Generation -->

  <!-- Model -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Model</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>YOLOv8s:</strong> Our top-performing model is based on a small variant of YOLOv8 developed by
            Ultralytics. We are training this pre-trained model, specifically using a resolution of 512x512 and
            exclusively employing images with bounding boxes. All other images, including unannotated ones and
            backgrounds without objects, are omitted from our training data. Notably, our dataset consists solely of
            non-synthetic images. Therefore, our achieved results are solely attributed to the quality of our dataset
            and the default settings of the model. Through our submission experiments, it's evident that the final score
            is heavily influenced by the size and quality of the training dataset. However, due to the constraints of
            time, we have not yet reached a definitive conclusion regarding our model's performance. Given more time for
            annotation and a larger training dataset, we anticipate that our score could improve from the current 0.48.
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Model -->

  <!-- Submission Experiments Table -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Submission Experiment</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>We conducted a comprehensive series of experiments, submitting a total of 30 entries. Here are select
            highlights:</p>
          <table class="table is-bordered is-hoverable">
            <thead>
              <tr>
                <th>Model</th>
                <th>Score</th>
                <th>Class Ratio</th>
                <th>Number of Images</th>
                <th>Note</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>YOLOv8n</td>
                <td>0.17</td>
                <td>(3379 : 371 : 69 : 12)</td>
                <td>250</td>
                <td>Training from pre-trained weight</td>
              </tr>
              <tr>
                <td>YOLOv8n</td>
                <td>0.28</td>
                <td>(4576 : 876 : 179 : 58)</td>
                <td>396</td>
                <td>Training from pre-trained weight</td>
              </tr>
              <tr>
                <td>YOLOv8n</td>
                <td>0.39</td>
                <td>(5995 : 1353 : 495 : 130)</td>
                <td>473</td>
                <td>Resuming training from previous best model (YOLOv8n score 0.28)</td>
              </tr>
              <tr>
                <td>YOLOv8s</td>
                <td>0.48</td>
                <td>(6986 : 1785 : 729 : 183)</td>
                <td>661</td>
                <td>Training from pre-trained weight</td>
              </tr>
            </tbody>
          </table>
          <p style="font-size: 12px;">Note: Class Ratio (Undamaged Residential Building : Damaged Residential Building :
            Undamaged Commercial Building : Damaged Commercial Building)</p>
          <p>Further experiments were conducted, including:</p>
          <ul style="list-style-type: none; padding-left: 0;">
            <li style="margin-bottom: 5px;"><strong>YOLOv8m:</strong> 0.33 - Medium Baseline</li>
            <li style="margin-bottom: 5px;"><strong>YOLOv8l:</strong> 0.39 - Large Baseline</li>
            <li style="margin-bottom: 5px;"><strong>YOLOv8s:</strong> 0.30 - YOLOv8s with synthetic dataset addition
            </li>
            <li style="margin-bottom: 5px;"><strong>YOLOv8s:</strong> 0.39 - YOLOv8s with reduced prediction confidence
              threshold to 0.28</li>
            <li style="margin-bottom: 5px;"><strong>YOLOv8s:</strong> 0.43 - YOLOv8s training with pre-trained weights
              and AdamW optimizer</li>
            <li style="margin-bottom: 5px;"><strong>YOLOv8s:</strong> 0.45 - Resuming training from previous best model
              (YOLOv8s score 0.48) with AdamW optimizer</li>
            <li style="margin-bottom: 5px;"><strong>YOLOv8s:</strong> 0.46 - Similar to the previous experiment with
              adjusted prediction threshold configurations</li>
          </ul>
          <p>From these experiments, we can conclude that our submission scores are mainly contributed by the quality of
            the dataset. We managed to achieve the highest score of 0.48 solely from the dataset itself. Our focus was
            primarily on preparing the training dataset, with minimal time allocated to model development. Given more
            time, we would explore various training configurations, model backbones, and architectures. Additionally, we
            would enhance our synthetic image generation models and collect more diverse datasets to further improve our
            results.</p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- End of Submission Experiments Table -->

  <!-- Conclusion -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Conclusion and Reflection</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <strong>1. Dataset is all you need:</strong> It is evident that everything comes down to how we prepare the
            dataset for training a machine learning model. No matter how deep the neural networks are or how
            sophisticated the model is, if the dataset is of poor quality, our model will never be able to make accurate
            predictions. Our experiments have shown that with a quality dataset, achieving a minimum of 0.40 is
            possible, which is sufficient to meet the minimum requirement by EY for a certificate of completion. Our
            team spent most of the time annotating the dataset and preparing a high-quality dataset. Each label was
            cross-referenced with reliable sources using Google Maps, OpenStreetMap, and even Google 3D street view to
            determine the class for each building.
          </p>
          <p>
            <strong>2. Start with a small model:</strong> It is advisable to begin with a small model when embarking on
            a machine learning project. When selecting a model for training, starting with the smallest and lightest
            model is preferable before moving on to larger and heavier models. The complexity of a model depends on its
            parameters and the depth of its layers. By starting with a small model, we have more flexibility to expand
            further with larger models. Our best model is the small variant of YOLOv8, and although we attempted to
            train YOLOv8m and YOLOv8l, we were unable to surpass our small model. Please note that these findings are
            based on our experiments, and other teams may obtain different results.
          </p>
          <p>
            <strong>3. Not everything goes your way:</strong> Despite our efforts to address the imbalanced dataset by
            generating synthetic data with Stable Diffusion 2, the results did not meet our expectations. We anticipated
            that resolving the imbalance would lead to improved model performance, but this was not the case. Several
            factors may have contributed to this outcome, including insufficient training of the stable diffusion model
            to generate synthetic data resembling the actual dataset, lack of generalization in the model, and limited
            variation in the synthetic dataset. It is important to remember that machine learning involves
            experimentation. By continually experimenting with different settings, architectures, and approaches, we may
            eventually build a better model through trial, error, and continuous testing and monitoring.
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Conclusion -->

  <!-- Logo Acknowledgment -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h4 class="title is-3" style="white-space: nowrap;">Technological Stack</h4>
      <div class="content has-text-justified">
        <div style="text-align: justify;">
          <p>
            <a href="https://www.python.org/" target="_blank"><img src="static/custom_icon/python-logo.svg" alt="python"
                style="width: 200px;"></a>
            <a href="https://github.com/ultralytics/ultralytics" target="_blank"><img
                src="static/custom_icon/ultralyticsyolo-logo.svg" alt="ultralytics" style="width: 200px;"></a>
            <a href="https://pytorch.org/" target="_blank"><img src="static/custom_icon/pytorch-logo.svg" alt="pytorch"
                style="width: 210px;"></a>
            <a href="https://www.kaggle.com/" target="_blank"><img src="static/custom_icon/kaggle-logo.png" alt="kaggle"
                style="width: 120px;"></a>
            <a href="https://huggingface.co/" target="_blank"><img src="static/custom_icon/hf-logo.svg"
                alt="huggingface" style="width: 80px;"></a>
            <a href="https://qgis.org/" target="_blank"><img src="static/custom_icon/qgis-logo.svg" alt="wandb"
                style="width: 120px;"></a>
            <a href="https://wandb.ai/" target="_blank"><img src="static/custom_icon/wand-logo.png" alt="wandb"
                style="width: 60px;"></a>
            <a href="https://github.com/pylabel-project/pylabel" target="_blank"><img
                src="static/custom_icon/pylabel-logo.png" alt="pylabel" style="width: 130px;"></a>
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>
  <!-- Logo Acknowledgment -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Default Statcounter code for EY project website
http://EY-Groupie2024WG.github.io -->
  <script type="text/javascript">
    var sc_project = 12976265;
    var sc_invisible = 1;
    var sc_security = "c70be6f1"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/12976265/0/c70be6f1/1/" alt="Web Analytics"
          referrerPolicy="no-referrer-when-downgrade"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

</body>

</html>
